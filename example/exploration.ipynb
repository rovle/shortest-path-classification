{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('shortest-path': conda)"
  },
  "interpreter": {
   "hash": "e4724c4f1193458327c9fd27e93e82201b4f22d46bfd685926396a0b7f3503db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Learning languages from a single message\n",
    "\n",
    "[...] First we import everything that we're going to be using throughout the notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from statistics import median, mean\n",
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from random import shuffle, sample, seed\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra\n",
    "\n",
    "if os.getcwd()[-7:] == 'example':\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from algorithm.base import ShortestPathModel\n",
    "\n",
    "from example.dataset_utils.sample_dataset import sample_dataset\n",
    "\n",
    "\n",
    "seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Differentiating between English, French, Italian and German\n",
    "\n",
    "For our first example, we're just going to take 4 languages (among 15 in our dataset) and see how well our algorithm works on differentiating messages of varying length in these languages. So let us first try it out on a simple example of a thousand messages from each of these languages, each 10 words long."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "num_samples = 2000\n",
    "samples = {\n",
    "    'english' : sample_dataset(num_samples, 10, 'en'),\n",
    "    'french' : sample_dataset(num_samples, 10, 'fr'),\n",
    "    'italian' : sample_dataset(num_samples, 10, 'it'),\n",
    "    'german' : sample_dataset(num_samples, 10, 'de')\n",
    "    }\n",
    "    \n",
    "languages = samples.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define the similarity score function. This function is the crux of this whole model, but it is very simple, owing to the fact that the domain is very simple. So, for two strings $s_1$ and $s_2$, we define \n",
    "$$\n",
    "\\operatorname{weight}(s_1, s_2, p)=\n",
    "\\begin{cases}\n",
    "\\text{(number of words shared by } s_1 \\text{ and } s_2 \\text{)}^{-p} & \\text{ if } s_1 \\text{ and } s_2 \\text{ share at least one word,}\\\\\n",
    "\\infty & \\text{ otherwise.}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "Here $\\infty$ is a shorthand for \"there is no edge between those two vertices.\"\n",
    "\n",
    "We also define one other helper function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def weight_function(string1, string2, p=2):\n",
    "    intersection = [x for x in string1 if x in string2]\n",
    "    if len(intersection) == 0:\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return 1/(len(intersection) ** p)\n",
    "\n",
    "def print_report(accuracy_list):\n",
    "    print(f\"\"\"Final report\n",
    "---------\n",
    "Mean accuracy is {100*mean(accuracy_list)}% and median is {100*median(accuracy_list)}%,\n",
    "minimum accuracy is {100*min(accuracy_list)}% and maximum accuracy is {100*max(accuracy_list)}%\n",
    "---------\"\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "accuracies = []\n",
    "\n",
    "for language_anchor, language_other in product(languages, repeat=2):\n",
    "    if language_anchor == language_other:\n",
    "        continue\n",
    "    print(f\"\"\"Using an example in {language_anchor.capitalize()} to learn the \n",
    "    difference between {language_anchor.capitalize()} and {language_other.capitalize()}...\"\"\")\n",
    "\n",
    "    model = ShortestPathModel(weight_function)\n",
    "\n",
    "    current_sample = samples[language_anchor] + samples[language_other]\n",
    "    labels = (len(samples[language_anchor]) * [1] +\n",
    "                len(samples[language_other]) * [0] )\n",
    "    n_of_labels = len(labels)\n",
    "    \n",
    "    model.fit_predict(current_sample)\n",
    "\n",
    "    # For calculating accuracy, take all but the first example, since\n",
    "    # that is the known one\n",
    "    accuracy = sum([1 if (labels[i] == model.predictions_on_train_set[i])\n",
    "                    else 0\n",
    "                    for i in range(n_of_labels)][1:]) / (n_of_labels - 1)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy: {100*accuracy}%\")\n",
    "    \n",
    "\n",
    "print_report(accuracies)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using an example in English to differentiate it from French...\n",
      "Accuracy: 99.17479369842461%\n",
      "Using an example in English to differentiate it from Italian...\n",
      "Accuracy: 96.42410602650664%\n",
      "Using an example in English to differentiate it from German...\n",
      "Accuracy: 99.4498624656164%\n",
      "Using an example in French to differentiate it from English...\n",
      "Accuracy: 90.49762440610152%\n",
      "Using an example in French to differentiate it from Italian...\n",
      "Accuracy: 95.04876219054765%\n",
      "Using an example in French to differentiate it from German...\n",
      "Accuracy: 97.07426856714179%\n",
      "Using an example in Italian to differentiate it from English...\n",
      "Accuracy: 91.02275568892223%\n",
      "Using an example in Italian to differentiate it from French...\n",
      "Accuracy: 84.0960240060015%\n",
      "Using an example in Italian to differentiate it from German...\n",
      "Accuracy: 91.02275568892223%\n",
      "Using an example in German to differentiate it from English...\n",
      "Accuracy: 87.44686171542885%\n",
      "Using an example in German to differentiate it from French...\n",
      "Accuracy: 99.12478119529882%\n",
      "Using an example in German to differentiate it from Italian...\n",
      "Accuracy: 87.72193048262066%\n",
      "Final report\n",
      "---------\n",
      "Mean accuracy is 93.17537717762774% and median is 93.03575893973493%,\n",
      "minimum accuracy is 84.0960240060015% and maximum accuracy is 99.4498624656164%\n",
      "---------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Playing with hyperparameters\n",
    "\n",
    "In the above example, we've seen that the model works well when given 1000 messages of length 10 in one language, and with the similarity function of inverse of the squared difference of the number of words shared. So there are a few things we can play with: the number of messages, their length, and the similarity function (in which, for simplicity, we'll only change the \"squared\" part).\n",
    "\n",
    "Let us then switch our attention just to French and Italian. While it does not speak much about the peculiarities of our own dataset, French and Italian do have a high [lexical similarity](https://en.wikipedia.org/wiki/Lexical_similarity#Indo-European_languages), making them as good choice as any among the $15\\cdot14/2=105$ options."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's try the same parameters again\n",
    "\n",
    "As our first experiment, let us try the same parameters but drawing many different samples from our dataset. When we draw 2000 samples of length 10 from our dataset we only draw about 20 thousand words, which is a small subset of about 3 millions words that are in each dataset, providing confidence there will be little correlation between our different runs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "num_of_runs = 50\n",
    "accuracies = []\n",
    "for _ in range(num_of_runs):\n",
    "    french = sample_dataset(num_samples, 10, 'fr')\n",
    "    italian = sample_dataset(num_samples, 10, 'it')\n",
    "    \n",
    "    model = ShortestPathModel(weight_function)\n",
    "\n",
    "    current_sample = french + italian\n",
    "    labels = (len(french) * [1] +\n",
    "                len(italian) * [0] )\n",
    "    n_of_labels = len(labels)\n",
    "    \n",
    "    model.fit_predict(current_sample)\n",
    "\n",
    "    # For calculating accuracy, take all but the first example, since\n",
    "    # that is the known one\n",
    "    accuracy = sum([1 if (labels[i] == model.predictions_on_train_set[i])\n",
    "                    else 0\n",
    "                    for i in range(n_of_labels)][1:]) / (n_of_labels - 1)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy: {100*accuracy}%\")\n",
    "\n",
    "print_report(accuracies)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 76.56914228557139%\n",
      "Accuracy: 93.02325581395348%\n",
      "Accuracy: 89.89747436859214%\n",
      "Accuracy: 77.56939234808702%\n",
      "Accuracy: 94.47361840460114%\n",
      "Accuracy: 93.09827456864215%\n",
      "Accuracy: 93.67341835458865%\n",
      "Accuracy: 84.6461615403851%\n",
      "Accuracy: 93.82345586396599%\n",
      "Accuracy: 94.19854963740936%\n",
      "Accuracy: 75.66891722930733%\n",
      "Accuracy: 91.12278069517379%\n",
      "Accuracy: 76.44411102775695%\n",
      "Accuracy: 74.56864216054014%\n",
      "Accuracy: 93.5233808452113%\n",
      "Accuracy: 84.92123030757689%\n",
      "Accuracy: 90.74768692173043%\n",
      "Accuracy: 92.54813703425856%\n",
      "Accuracy: 93.74843710927732%\n",
      "Accuracy: 93.64841210302576%\n",
      "Accuracy: 91.07276819204802%\n",
      "Accuracy: 94.87371842960741%\n",
      "Accuracy: 94.42360590147537%\n",
      "Accuracy: 93.62340585146288%\n",
      "Accuracy: 80.64516129032258%\n",
      "Accuracy: 94.24856214053513%\n",
      "Accuracy: 92.39809952488122%\n",
      "Accuracy: 90.67266816704176%\n",
      "Accuracy: 94.5986496624156%\n",
      "Accuracy: 91.74793698424605%\n",
      "Accuracy: 92.39809952488122%\n",
      "Accuracy: 94.72368092023005%\n",
      "Accuracy: 92.27306826706678%\n",
      "Accuracy: 87.99699924981246%\n",
      "Accuracy: 76.5441360340085%\n",
      "Accuracy: 96.19904976244061%\n",
      "Accuracy: 91.04776194048512%\n",
      "Accuracy: 75.11877969492373%\n",
      "Accuracy: 88.77219304826205%\n",
      "Accuracy: 93.09827456864215%\n",
      "Accuracy: 95.17379344836209%\n",
      "Accuracy: 78.36959239809953%\n",
      "Accuracy: 74.94373593398349%\n",
      "Accuracy: 96.07401850462615%\n",
      "Accuracy: 91.82295573893472%\n",
      "Accuracy: 77.3693423355839%\n",
      "Accuracy: 95.1487871967992%\n",
      "Accuracy: 91.69792448112028%\n",
      "Accuracy: 95.1487871967992%\n",
      "Accuracy: 76.49412353088272%\n",
      "Final report\n",
      "---------\n",
      "Mean accuracy is 88.7326831707927% and median is 92.04801200300075%,\n",
      "minimum accuracy is 74.56864216054014% and maximum accuracy is 96.19904976244061%\n",
      "---------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}