{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('shortest-path': conda)"
  },
  "interpreter": {
   "hash": "e4724c4f1193458327c9fd27e93e82201b4f22d46bfd685926396a0b7f3503db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Learning languages from a single message\n",
    "\n",
    "[...] First we import everything that we're going to be using throughout the notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from statistics import median, mean\n",
    "from itertools import combinations, product\n",
    "from collections import defaultdict\n",
    "from random import shuffle, sample, seed\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra\n",
    "\n",
    "if os.getcwd()[-7:] == 'example':\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from algorithm.base import ShortestPathModel\n",
    "\n",
    "from example.dataset_utils.sample_dataset import sample_dataset\n",
    "\n",
    "\n",
    "seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Differentiating between English, French, Italian and German\n",
    "\n",
    "For our first example, we're just going to take 4 languages (among 15 in our dataset) and see how well our algorithm works on differentiating messages of varying length in these languages. So let us first try it out on a simple example of a thousand messages from each of these languages, each 10 words long."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "num_samples = 2000\n",
    "samples = {\n",
    "    'english' : sample_dataset(num_samples, 10, 'en'),\n",
    "    'french' : sample_dataset(num_samples, 10, 'fr'),\n",
    "    'italian' : sample_dataset(num_samples, 10, 'it'),\n",
    "    'german' : sample_dataset(num_samples, 10, 'de')\n",
    "    }\n",
    "    \n",
    "languages = samples.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define the similarity score function. This function is the crux of this whole model, but it is very simple, owing to the fact that the domain is very simple. So, for two strings $s_1$ and $s_2$, we define \n",
    "$$\\operatorname{weight_function}(s_1, s_2) =  \\begin{cases}\n",
    "      0, & \\text{if}\\ a=1 \\\\\n",
    "      1, & \\text{otherwise}\n",
    "    \\end{cases}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def weight_function(string1, string2):\n",
    "    intersection = [x for x in string1 if x in string2]\n",
    "    if len(intersection) == 0:\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return 1/(len(intersection) ** 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "accuracies = []\n",
    "\n",
    "for language_anchor, language_other in product(languages, repeat=2):\n",
    "    if language_anchor == language_other:\n",
    "        continue\n",
    "    print(f'Using an example in {language_anchor.capitalize()} to differentiate it from {language_other.capitalize()}...')\n",
    "\n",
    "    model = ShortestPathModel(weight_function)\n",
    "\n",
    "    current_sample = samples[language_anchor] + samples[language_other]\n",
    "    labels = (len(samples[language_anchor]) * [1] +\n",
    "                len(samples[language_other]) * [0] )\n",
    "    n_of_labels = len(labels)\n",
    "    \n",
    "    model.fit_predict(current_sample)\n",
    "\n",
    "    # For calculating accuracy, take all but the first example, since\n",
    "    # that is the known one\n",
    "    accuracy = sum([1 if (labels[i] == model.predictions_on_train_set[i])\n",
    "                    else 0\n",
    "                    for i in range(n_of_labels)][1:]) / (n_of_labels - 1)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy: {100*accuracy}%\")\n",
    "    \n",
    "print(f\"\"\"Final report\n",
    "---------\n",
    "Mean accuracy is {100*mean(accuracies)}% and median is {100*median(accuracies)}%,\n",
    "minimum accuracy is {100*min(accuracies)}% and maximum accuracy is {100*max(accuracies)}%\n",
    "---------\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using an example in English to differentiate it from French...\n",
      "Accuracy: 99.17479369842461%\n",
      "Using an example in English to differentiate it from Italian...\n",
      "Accuracy: 96.42410602650664%\n",
      "Using an example in English to differentiate it from German...\n",
      "Accuracy: 99.4498624656164%\n",
      "Using an example in French to differentiate it from English...\n",
      "Accuracy: 90.49762440610152%\n",
      "Using an example in French to differentiate it from Italian...\n",
      "Accuracy: 95.04876219054765%\n",
      "Using an example in French to differentiate it from German...\n",
      "Accuracy: 97.07426856714179%\n",
      "Using an example in Italian to differentiate it from English...\n",
      "Accuracy: 91.02275568892223%\n",
      "Using an example in Italian to differentiate it from French...\n",
      "Accuracy: 84.0960240060015%\n",
      "Using an example in Italian to differentiate it from German...\n",
      "Accuracy: 91.02275568892223%\n",
      "Using an example in German to differentiate it from English...\n",
      "Accuracy: 87.44686171542885%\n",
      "Using an example in German to differentiate it from French...\n",
      "Accuracy: 99.12478119529882%\n",
      "Using an example in German to differentiate it from Italian...\n",
      "Accuracy: 87.72193048262066%\n",
      "Final report\n",
      "---------\n",
      "Mean accuracy is 93.17537717762774% and median is 93.03575893973493%,\n",
      "minimum accuracy is 84.0960240060015% and maximum accuracy is 99.4498624656164%\n",
      "---------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Playing with hyperparameters\n",
    "\n",
    "In the above example, we've seen that the model works well when given 1000 messages of length 10 in one language, and with the similarity function of inverse of the squared difference of the number of words shared. So there are a few things we can play with: the number of messages, their length, and the similarity function (in which, for simplicity, we'll only change the \"squared\" part).\n",
    "\n",
    "Let us then switch our attention just to French and Italian. While it does not speak much about the peculiarities of our own dataset, French and Italian do have a high [lexical similarity](https://en.wikipedia.org/wiki/Lexical_similarity#Indo-European_languages), making them as good choice as any among the $15\\cdot14/2=105$ options."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}